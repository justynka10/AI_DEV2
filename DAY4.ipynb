{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykłady"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auth import openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cześć Justynko! Jak się masz? W czym mogę Ci dzisiaj pomóc?\n"
     ]
    }
   ],
   "source": [
    "# 01_langchain_init\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Inicjalizacja domyślnego modelu, czyli gpt-3.5-turbo\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "# Wywołanie modelu poprzez przesłanie tablicy wiadomości.\n",
    "# W tym przypadku to proste przywitanie\n",
    "message = HumanMessage(\"Hej, jestem Justynka!\")\n",
    "response = chat.invoke([message])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nie wiem\n"
     ]
    }
   ],
   "source": [
    "# 02_langchain_format\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.context import context\n",
    "\n",
    "# Treść kontekstu\n",
    "context = \"\"\"\n",
    "Vercel AI SDK to biblioteka typu open-source zaprojektowana, aby pomóc programistom tworzyć interfejsy użytkownika dla konwersacji, streamingu i czatu w JavaScript i TypeScript. SDK wspiera React/Next.js, Svelte/SvelteKit, przy czym wsparcie dla Nuxt/Vue będzie dostępne wkrótce.\n",
    "Aby zainstalować SDK, wprowadź następujące polecenie w terminalu:\n",
    "npm install ai\n",
    "\"\"\"\n",
    "\n",
    "# Zwykle do definiowania promptów warto korzystać z template strings\n",
    "# Tutaj treści zamknięte w klamrach {} są zastępowane przez LangChain konkretnymi wartościami\n",
    "system_template = \"\"\"\n",
    "As a {role} who answers the questions ultra-concisely using CONTEXT below \n",
    "and nothing more and truthfully says \"don't know\" when the CONTEXT is not enough to give an answer.\n",
    "\n",
    "context###{context}###\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{text}\"\n",
    "\n",
    "# Lista wiadomości\n",
    "messages = [\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "]\n",
    "\n",
    "# Inicjalizacja promptu\n",
    "chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Faktyczne uzupełnienie szablonów wartościami\n",
    "formatted_chat_prompt = chat_prompt.format_messages(\n",
    "    context= context,\n",
    "    role= \"Doświadczony programista JavaScript\",\n",
    "    text= \"Co to wyrażenia lambda w Pythonie?\",\n",
    ")\n",
    "\n",
    "# Inicjalizacja domyślnego modelu, czyli gpt-3.5-turbo\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "# Wykonanie zapytania do modelu\n",
    "response =  chat.invoke(formatted_chat_prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku definiowania contextu i systemowej templatki należy używać języka ANGIELSKIEGO. Jeśli robię to po polsku, to odpowiedzi są niezgodne z poleceniem - AI nie używa odpowiedzi \"nie wiem\". Po zdefiniowaniu wytycznych po angielsku można zadawac pytanie zarówno po angielsku jak i po polsku i odpowiada dobrze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|C|ze|ść| Just|yn|ko|!| Jak| mog|ę| Ci| dz|isia|j| pom|ó|c|?||"
     ]
    }
   ],
   "source": [
    "# 03_langchain_stream\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "## Inicjalizacja chatu z włączonym streamingiem\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=openai_api_key, \n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Wywołanie chatu wraz z funkcją przyjmującą kolejne tokeny składające się na wypowiedź modelu\n",
    "chunks = []\n",
    "async for chunk in chat.astream(\"Cześć, jestem Justynka!\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "więcej o streamingu tutaj: https://python.langchain.com/docs/expression_language/streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadania praktyczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.task_service import get_authentication_token, get_task, send_answer\n",
    "from auth import apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Automatycznie wyszukuje plik .env w katalogu bieżącym i załaduje zmienne środowiskowe\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otrzymano token: 8aaef3d04af75f29ce34722db8627efff495a4f8\n"
     ]
    }
   ],
   "source": [
    "# autoryzacja\n",
    "\n",
    "task_name = \"moderation\"\n",
    "\n",
    "token = get_authentication_token(apikey, task_name)\n",
    "if token:\n",
    "    print(\"Otrzymano token:\", token)\n",
    "else:\n",
    "    print(\"Nie udało się uzyskać tokenu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź z serwera: {'code': 0, 'msg': 'please return 0/1 for each sentence to classify if this is content that requires moderation', 'input': ['azjaci są głupi i brzydcy i nie powinni żyć', 'ten gość musi zginąć. Nie pozwole sobię na obrażanie mnie.', 'Sasha.Grey.s3x.p0rn.extreme-interracial.S03E12.DVDRip.mp4', 'majonez Winiary jest lepszy od Kieleckiego']}\n"
     ]
    }
   ],
   "source": [
    "# pobranie danych do zadania\n",
    "\n",
    "response = get_task(token)\n",
    "if response:\n",
    "    print(\"Odpowiedź z serwera:\", response)\n",
    "else:\n",
    "    print(\"Wystąpił błąd podczas wysyłania zapytania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azjaci są głupi i brzydcy i nie powinni żyć', 'ten gość musi zginąć. Nie pozwole sobię na obrażanie mnie.', 'Sasha.Grey.s3x.p0rn.extreme-interracial.S03E12.DVDRip.mp4', 'majonez Winiary jest lepszy od Kieleckiego']\n"
     ]
    }
   ],
   "source": [
    "print(response['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "answers = []\n",
    "for statement in response['input']:\n",
    "    response = client.moderations.create(input=statement)\n",
    "    \n",
    "    output = response.results[0]\n",
    "    if(output.flagged) == True:\n",
    "        answers.append(1)\n",
    "    else:\n",
    "        answers.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź z serwera: {'code': 0, 'msg': 'OK', 'note': 'CORRECT'}\n"
     ]
    }
   ],
   "source": [
    "# wysłanie odpowiedzi\n",
    "\n",
    "response = send_answer(token, answers)\n",
    "if response:\n",
    "    print(\"Odpowiedź z serwera:\", response)\n",
    "else:\n",
    "    print(\"Wystąpił błąd podczas wysyłania odpowiedzi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otrzymano token: 26835f56f166a7639cb6ed45f71caf6ff18f71be\n"
     ]
    }
   ],
   "source": [
    "# autoryzacja\n",
    "\n",
    "task_name = \"blogger\"\n",
    "\n",
    "token = get_authentication_token(apikey, task_name)\n",
    "if token:\n",
    "    print(\"Otrzymano token:\", token)\n",
    "else:\n",
    "    print(\"Nie udało się uzyskać tokenu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź z serwera: {'code': 0, 'msg': 'please write blog post for the provided outline', 'blog': ['Wstęp: kilka słów na temat historii pizzy', 'Niezbędne składniki na pizzę', 'Robienie pizzy', 'Pieczenie pizzy w piekarniku']}\n"
     ]
    }
   ],
   "source": [
    "# pobranie danych do zadania\n",
    "\n",
    "response = get_task(token)\n",
    "if response:\n",
    "    print(\"Odpowiedź z serwera:\", response)\n",
    "else:\n",
    "    print(\"Wystąpił błąd podczas wysyłania zapytania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wstęp: kilka słów na temat historii pizzy', 'Niezbędne składniki na pizzę', 'Robienie pizzy', 'Pieczenie pizzy w piekarniku']\n"
     ]
    }
   ],
   "source": [
    "print(response['blog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
