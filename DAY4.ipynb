{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykłady"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auth import openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cześć Justynko! Jak się masz? W czym mogę Ci dzisiaj pomóc?\n"
     ]
    }
   ],
   "source": [
    "# 01_langchain_init\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Inicjalizacja domyślnego modelu, czyli gpt-3.5-turbo\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "# Wywołanie modelu poprzez przesłanie tablicy wiadomości.\n",
    "# W tym przypadku to proste przywitanie\n",
    "message = HumanMessage(\"Hej, jestem Justynka!\")\n",
    "response = chat.invoke([message])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nie wiem\n"
     ]
    }
   ],
   "source": [
    "# 02_langchain_format\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.context import context\n",
    "\n",
    "# Treść kontekstu\n",
    "context = \"\"\"\n",
    "Vercel AI SDK to biblioteka typu open-source zaprojektowana, aby pomóc programistom tworzyć interfejsy użytkownika dla konwersacji, streamingu i czatu w JavaScript i TypeScript. SDK wspiera React/Next.js, Svelte/SvelteKit, przy czym wsparcie dla Nuxt/Vue będzie dostępne wkrótce.\n",
    "Aby zainstalować SDK, wprowadź następujące polecenie w terminalu:\n",
    "npm install ai\n",
    "\"\"\"\n",
    "\n",
    "# Zwykle do definiowania promptów warto korzystać z template strings\n",
    "# Tutaj treści zamknięte w klamrach {} są zastępowane przez LangChain konkretnymi wartościami\n",
    "system_template = \"\"\"\n",
    "As a {role} who answers the questions ultra-concisely using CONTEXT below \n",
    "and nothing more and truthfully says \"don't know\" when the CONTEXT is not enough to give an answer.\n",
    "\n",
    "context###{context}###\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{text}\"\n",
    "\n",
    "# Lista wiadomości\n",
    "messages = [\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "]\n",
    "\n",
    "# Inicjalizacja promptu\n",
    "chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Faktyczne uzupełnienie szablonów wartościami\n",
    "formatted_chat_prompt = chat_prompt.format_messages(\n",
    "    context= context,\n",
    "    role= \"Doświadczony programista JavaScript\",\n",
    "    text= \"Co to wyrażenia lambda w Pythonie?\",\n",
    ")\n",
    "\n",
    "# Inicjalizacja domyślnego modelu, czyli gpt-3.5-turbo\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "# Wykonanie zapytania do modelu\n",
    "response =  chat.invoke(formatted_chat_prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku definiowania contextu i systemowej templatki należy używać języka ANGIELSKIEGO. Jeśli robię to po polsku, to odpowiedzi są niezgodne z poleceniem - AI nie używa odpowiedzi \"nie wiem\". Po zdefiniowaniu wytycznych po angielsku można zadawac pytanie zarówno po angielsku jak i po polsku i odpowiada dobrze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|C|ze|ść| Just|yn|ko|!| Jak| mog|ę| Ci| dz|isia|j| pom|ó|c|?||"
     ]
    }
   ],
   "source": [
    "# 03_langchain_stream\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "## Inicjalizacja chatu z włączonym streamingiem\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=openai_api_key, \n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Wywołanie chatu wraz z funkcją przyjmującą kolejne tokeny składające się na wypowiedź modelu\n",
    "chunks = []\n",
    "async for chunk in chat.astream(\"Cześć, jestem Justynka!\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "więcej o streamingu tutaj: https://python.langchain.com/docs/expression_language/streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04_tiktoken\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "from tiktoken import get_encoding\n",
    "\n",
    "# Definicja typu Message zgodnie z podaną definicją\n",
    "Message = Dict[str, Optional[str]]\n",
    "\n",
    "def count_tokens(messages: List[Message], model=\"gpt-3.5-turbo-0613\") -> int:\n",
    "    encoding = get_encoding(\"cl100k_base\")\n",
    "\n",
    "    tokens_per_message, tokens_per_name = 0, 0\n",
    "    if model in [\"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k-0613\", \"gpt-4-0314\", \"gpt-4-32k-0314\", \"gpt-4-0613\", \"gpt-4-32k-0613\"]:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4\n",
    "        tokens_per_name = -1\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return count_tokens(messages, \"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return count_tokens(messages, \"gpt-4-0613\")\n",
    "    else:\n",
    "        raise ValueError(f\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\")\n",
    "\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            if value is not None:  # Upewniamy się, że przetwarzamy tylko istniejące wartości\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":\n",
    "                    num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "Token Count: 16\n",
      "Token IDs: [34, 3059, 25898, 13599, 336, 4702, 1910, 4657, 0]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from tiktoken import get_encoding\n",
    "\n",
    "# Definicja Message zgodnie z wcześniejszym przykładem\n",
    "Message = dict\n",
    "\n",
    "# Przykładowa lista wiadomości\n",
    "messages: List[Message] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Cześć jestem Justynka!\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Wywołanie funkcji countTokens i wydruk wyniku\n",
    "num = count_tokens(messages, 'gpt-4')  # 11 to przykładowy wynik, tutaj wywołujemy funkcję\n",
    "print(f\"Token Count: {num}\")\n",
    "\n",
    "# Użycie get_encoding do zakodowania treści pierwszej wiadomości\n",
    "encoding = get_encoding(\"cl100k_base\")\n",
    "print(f\"Token IDs: {encoding.encode(messages[0]['content'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadania praktyczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.task_service import get_authentication_token, get_task, send_answer\n",
    "from auth import apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Automatycznie wyszukuje plik .env w katalogu bieżącym i załaduje zmienne środowiskowe\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otrzymano token: 8aaef3d04af75f29ce34722db8627efff495a4f8\n"
     ]
    }
   ],
   "source": [
    "# autoryzacja\n",
    "\n",
    "task_name = \"moderation\"\n",
    "\n",
    "token = get_authentication_token(apikey, task_name)\n",
    "if token:\n",
    "    print(\"Otrzymano token:\", token)\n",
    "else:\n",
    "    print(\"Nie udało się uzyskać tokenu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź z serwera: {'code': 0, 'msg': 'please return 0/1 for each sentence to classify if this is content that requires moderation', 'input': ['azjaci są głupi i brzydcy i nie powinni żyć', 'ten gość musi zginąć. Nie pozwole sobię na obrażanie mnie.', 'Sasha.Grey.s3x.p0rn.extreme-interracial.S03E12.DVDRip.mp4', 'majonez Winiary jest lepszy od Kieleckiego']}\n"
     ]
    }
   ],
   "source": [
    "# pobranie danych do zadania\n",
    "\n",
    "response = get_task(token)\n",
    "if response:\n",
    "    print(\"Odpowiedź z serwera:\", response)\n",
    "else:\n",
    "    print(\"Wystąpił błąd podczas wysyłania zapytania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azjaci są głupi i brzydcy i nie powinni żyć', 'ten gość musi zginąć. Nie pozwole sobię na obrażanie mnie.', 'Sasha.Grey.s3x.p0rn.extreme-interracial.S03E12.DVDRip.mp4', 'majonez Winiary jest lepszy od Kieleckiego']\n"
     ]
    }
   ],
   "source": [
    "print(response['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "answers = []\n",
    "for statement in response['input']:\n",
    "    response = client.moderations.create(input=statement)\n",
    "    \n",
    "    output = response.results[0]\n",
    "    if(output.flagged) == True:\n",
    "        answers.append(1)\n",
    "    else:\n",
    "        answers.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź z serwera: {'code': 0, 'msg': 'OK', 'note': 'CORRECT'}\n"
     ]
    }
   ],
   "source": [
    "# wysłanie odpowiedzi\n",
    "\n",
    "response = send_answer(token, answers)\n",
    "if response:\n",
    "    print(\"Odpowiedź z serwera:\", response)\n",
    "else:\n",
    "    print(\"Wystąpił błąd podczas wysyłania odpowiedzi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otrzymano token\n"
     ]
    }
   ],
   "source": [
    "# autoryzacja\n",
    "\n",
    "task_name = \"blogger\"\n",
    "\n",
    "token = get_authentication_token(apikey, task_name)\n",
    "if token:\n",
    "    print(\"Otrzymano token\")\n",
    "else:\n",
    "    print(\"Nie udało się uzyskać tokenu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź z serwera: {'code': 0, 'msg': 'please write blog post for the provided outline', 'blog': ['Wstęp: kilka słów na temat historii pizzy', 'Niezbędne składniki na pizzę', 'Robienie pizzy', 'Pieczenie pizzy w piekarniku']}\n"
     ]
    }
   ],
   "source": [
    "# pobranie danych do zadania\n",
    "\n",
    "response = get_task(token)\n",
    "if response:\n",
    "    print(\"Odpowiedź z serwera:\", response)\n",
    "else:\n",
    "    print(\"Wystąpił błąd podczas wysyłania zapytania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wstęp: kilka słów na temat historii pizzy', 'Niezbędne składniki na pizzę', 'Robienie pizzy', 'Pieczenie pizzy w piekarniku']\n"
     ]
    }
   ],
   "source": [
    "print(response['blog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(context, prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            temperature = 0.7,\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": context},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = []\n",
    "section_done = []\n",
    "\n",
    "context = \"\"\"You are an assistant to create articles for the website. You don't create the whole article right away, but the user sends you the headline of the section to write, and you create it.In addition, the user sends you a list of headings from the article that have already been written. Don't duplicate the information they focused on. Some tips you should follow:\n",
    "    - focus on creating content that is valuable and well thought out;\n",
    "    - avoid plagiarism and be creative. Create unique content that will make your text stand out;\n",
    "    - learn the basics of search engine optimization (SEO) and skillfully use keywords to increase your content's visibility in search results;\n",
    "    - when writing, be clear and concise. Avoid unnecessary words and complex sentence constructions that can put off readers;\n",
    "    - use bullets and highlighting to make your content easier to read and navigate. Good structure helps readers quickly find what they are looking for;\n",
    "    - encourage interaction through questions or calls to action; \n",
    "    - use a friendly, conversational tone;\"\"\"\n",
    "\n",
    "for section in response['blog']:\n",
    "    prompt = f\"Stwórz treść sekcji: {section}. Rozwinąłeś już wcześniej takie sekcje: {section_done}.\"\n",
    "\n",
    "    section_text = text_generator(context, prompt)\n",
    "    section_done.append(section)\n",
    "    article.append(section_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź z serwera: {'code': 0, 'msg': 'OK', 'note': 'CORRECT'}\n"
     ]
    }
   ],
   "source": [
    "# wysłanie odpowiedzi\n",
    "\n",
    "response = send_answer(token, article)\n",
    "if response:\n",
    "    print(\"Odpowiedź z serwera:\", response)\n",
    "else:\n",
    "    print(\"Wystąpił błąd podczas wysyłania odpowiedzi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
